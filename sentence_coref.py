# -*- coding: utf-8 -*-
"""Copy of attribute extraction idea.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13dX5g01ONM0rC_lZTavS7z8s2bk28LHU
"""

!pip install allennlp==2.1.0 allennlp-models==2.1.0

from allennlp.predictors.predictor import Predictor
from allennlp.data.tokenizers.whitespace_tokenizer import WhitespaceTokenizer
import allennlp_models.tagging

# predictor = Predictor.from_path("https://storage.googleapis.com/allennlp-public-models/biaffine-dependency-parser-ptb-2020.04.06.tar.gz")

coref_predictor = Predictor.from_path("https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz")

from nltk.tokenize import sent_tokenize, word_tokenize
import json
from tqdm.notebook import tqdm
import re
import math

category = "superheroes"

file = open(category+'_data.json', 'r')
json_data = json.load(file)
file.close()

def process_text(text):
    t = text
    for i in range(6, 0, -1):
        search = "={"+str(i)+"}(.*)={"+str(i)+"}"
        res = re.findall(search, t)
        for r in res:
            t = re.sub(r, "", t)
        t = re.sub("={"+str(2*i)+"}", "", t)
    t = re.sub('\n+', ' ', t)
    return t

for i in json_data:
  try:
    i['processed_text'] = process_text(i['article'])
  except:
    i['processed_text'] = i['article']

def find_main_cluster(tokens, clusters, words_in_title):
  for cluster in clusters:
    for span in cluster:
      words = tokens[span[0]:span[1] + 1]
      for word_in_title in words_in_title:
        if word_in_title in words:
          return cluster
  
  # code should not reach here in ideal case
  if len(clusters) > 0:
    return clusters[0]
  return []

coref_sents = {}
output_file = 'output.json'

num = 0
MAX_ARTICLES = 50

tokenizer = WhitespaceTokenizer()

for d in json_data:
  text = d['processed_text']
  title = d['title']
  words_in_title = d['title'].split(' ')
  sents = sent_tokenize(text)
  coref_sents[title] = []
  
  number_of_setences = len(sents)
  iters = math.ceil(number_of_setences / 50)
  
  for iter in range(iters):
    # find corefs for 50 sentences at a time
    cur_sents = sents[iter * 50 : iter * 50 + 50]
    cur_text = (' ').join(cur_sents)
    coref_result = coref_predictor.predict(document = cur_text)
    clusters = coref_result['clusters']
    tokenized_sentence = coref_result['document']
    cur_text = (' ').join(tokenized_sentence)
    cur_sents = sent_tokenize(cur_text)

    # find the cluster which refers to the title of the article
    main_cluster = find_main_cluster(tokenized_sentence, clusters, words_in_title)
    starting_indices = []
    for span in main_cluster:
      starting_indices.append(span[0])
    
    # store sentences which include a reference to the title of the article. Rest all sentences are ignored as we
    # assume that they will not contain any information about the title of the article
    sentence_start_index = 0
    span_index = 0
    total_spans = len(starting_indices)

    for sentence in cur_sents:
      words = tokenizer.tokenize(sentence)
      sentence_end_index = sentence_start_index + len(words)
      while span_index < total_spans and starting_indices[span_index] < sentence_start_index:
        span_index += 1

      if span_index < total_spans and starting_indices[span_index] < sentence_end_index:
        coref_sents[title].append(sentence)
        span_index += 1

      sentence_start_index += len(words)

    num += 1
    if num == MAX_ARTICLES:
      num = 0
      with open(output_file, 'w') as file:
        file.write(json.dumps(coref_sents))

with open(output_file, 'w') as file:
  file.write(json.dumps(coref_sents))

